{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/blog/62-acing-your-first-on-call-schedule-clear-report-on-every-second/",
    "result": {"data":{"site":{"siteMetadata":{"title":"satriahrh"}},"markdownRemark":{"id":"4b9f08c1-c72c-53a5-bbb9-074cdb624698","excerpt":"Not every company scheduling an on-call schedule for its software engineers. Some of them rallying on their SREs, regardless of the controversy I want to more…","html":"<p>Not every company scheduling an on-call schedule for its software engineers. Some of them rallying on their SREs, regardless of the controversy I want to more focus on you. You who are scheduled for an on-call.</p>\n<p>Just letting you know, you can skip this paragraph if you don’t want to hear to side story behind the writing of this article, when I first thought of writing “Acing Your First On Call Schedule”, I came up with idea of stating all the thing you need to concern and prepare. At the time writing it, I ended up being so nostalgic and the 4-5 article become a chapter of a book, exaggerated. Though, it is true that the article gone long. Then I came up with another idea to slicing it up. And this is one of the point of all of the concern and preparation you should know on Acing Your First On Call Schedule.</p>\n<h2>Reporting is Important and Urgent</h2>\n<p>The famous Eisenhower matrix told us that by using 4 quadrants you can leverage the activity you are/will doing, based on the importance and urgency. How to prioritize which activity should we do now based on the quadrant, is not our topic here and there are numerous backgrounds on conducting particular different prioritization methods by individuals.</p>\n<p>Reporting everything during an incident is both very important and very urgent, fall on to quadrant 1.</p>\n<p>Most people would agree that everything that falls on to quadrant 1 should be done first, no matter what prioritization method you are currently using. Because something urgent is not a thing we can delay, maybe someone is waiting outside the toilet for a pee then shall we not to delay if have finished our business. Every urgent matter doesn’t need to be important however, you can skip that urgent not important activity like a sharing session from your colleague in a few minutes for another important activity.</p>\n<p>Reporting an incident shall not be delayed, because it is urgent to tell the related stakeholder and teams what is going on with the system. It is also important, to make them know what is going on.</p>\n<p>Skipping a report and keeping it to yourself resulting in unaligned on your team. Someone may be wondering about a plummet on the GMV, and they might desperately try to understand that degradation. Giving them a minimum notice would certainly helpful for future understanding, they will explain to the client or stakeholder about the degradation of the system a few moments ago. It is also helpful for current understanding, they can conduct the best quick mitigation ever possible.</p>\n<h2>When to Report?</h2>\n<p>Remember that this is all based on my experience. I had not been in an engineer incident numerous times, neither I been an on-call engineer multiple times since the year of my experience in this professional won’t be sufficient enough for a good judgment. I just want to make an aspiration of how things should be done, in this particular case: reporting an incident when you are the on-call engineer.</p>\n<p>You are obligated to report the incident <strong>now</strong> and <strong>later</strong>. Now for when the incident is happening, later for when the incident has been mitigated and you have been recovered from exhaustion.</p>\n<p>About now and later are not an option for one, in fact, it uses <strong>and</strong> to state both of the condition should be met.</p>\n<p>In the previous section, I have told you that reporting the incident as quickly as possible resulting for the team and impacted to have current understanding. Probably, the mitigation is not from you as an on-call engineer. It might require a toggle off of some features resulting in to complete shut down of the feature, whereas letting the incident currently happening might be the best option for the business team since there are some users who still manage to use the feature without hassle. The mitigation might not be on your engineering team, rather from the related team or even the vendor. You may report the incident to all of the engineers in the whole internal organization, who knows the root cause if from the other team and they are currently on the way to deploy the hotfix. You may report the incident to a vendor, partner, or a third party, they have the right to know why all of sudden the traffic is vanishing from our service.</p>\n<p>I also have told you in the previous section, the reporting incidents will also be helpful for future understanding. The business has been aware of the incident that happened, they all have been agreed on the mitigation. However, they certainly want to avoid similar incidents. Asking the on-call engineer in charge when on the moment the incident is happening might not a good choice for the business team, and it is not a good idea either for both side since all of us are having the mindset to recover from the incident as quickly as possible before the impacted user growing up and bad publication start to trending. This is when I should introduce a new term, <strong>post mortem document</strong>, a report document of an incident written after the incident.</p>\n<h2>How to Report Now?</h2>\n<p>You should now understand that you should report the incident on the moment the incident is happening and after the incident happened. Then we are talking about how to reporting the incident, both now and later in a timely manner. However, it is not wise for me to write it down in the same section since the context of each event is completely different.</p>\n<p>Reporting the incident now required several points:</p>\n<ol>\n<li>Quick in reporting founding;</li>\n<li>Quick in generating mitigation;</li>\n<li>Concise in content but not eliminating any important details;</li>\n<li>An environment to gather a clear focus.</li>\n</ol>\n<p>The practice of reporting now is numerous and I still can’t argue which one is the best objectively. What I can do is state what I ever experienced and what is my favorite.</p>\n<p><strong>production incident report channel/group</strong> on usual organization chat platform is underrated. It is very traditional since the beginning of the messaging platform ever founded, it is very easy to report since we tend to take aside our interpersonal moral on virtual. So reporting on a chat is very easy. However I might give you a tip to make the report be more clean and clear, I am not wanting you just make an anarchy report channel to be cluttered with numerous report incidents at the same time.</p>\n<p><strong>the virtual war room</strong> is my favorite, even though I never been there at all, not at the big one. It is obviously different from the real war room, where all the participants are needed to gather into a big room and everyone shouting each other :p However, a virtual war room is much more clean and easy to create. Comparing to the chat report channel, the virtual war room also gives you more focus, quick discussion to mitigation, and easier for us to explain in oral.</p>\n<h2>How to Report Later?</h2>\n<p>Write a proper post mortem document. It should give the details of the minutes of the incident, the founding, the root causes, the mitigation, the action item, and the calculated impact from the incident. Although some organization have different point of view in what to report, but having all of those is a minimum.</p>\n<p>It is the best way to report for later. All of the post mortem documents on the organization should be gathered in one place, or in a way everyone can read it. There is still controversy about considering it as a restricted knowledge, so not everyone in the organization is allowed to read a “critical and sensitive” document detailing a bug in our system. I don’t want to make an argument about it, the thing is the related team and stakeholders should have easy access to the post mortem document. The why might be a good topic for my next article though, for now, you can simply take that it that way.</p>\n<p>However, the practice of creating a document is not enough. even make it easily accessible for related persons. The post mortem document should be the initial or the beginning of delivering better products to customer/user/client. Several lessons can be learned, several decisions can be made for better delivery. Who doesn’t want better delivery for our customer? Better traffic, more customer happy, more good publication, more revenue, more salary, more bonuses. I am just trying to telling you that it is very intuitive to make post mortem document as your judgement to refine your current service and build future service.</p>\n<h2>Story Time</h2>\n<p>This is pretty well known if you are been there, or you might have been told by someone there. It is a good story from a sad story. Hopefully, this is can help you understand of why reporting is both urgent and important, and how a good reporting can impact significantly better.</p>\n<p>For short, a big incident were happening. Every on call engineer got invited to a virtual war room, all night long.</p>\n<p>My luck that day wasn’t my schedule or unfortunate maybe? It was 8 or 9 pm when the incident first happened. Those hours are already pretty late for everyone to be still at work hour. It is when everyone has focused on their fun after one exhausting day. Slack app has been turned off for notification, so no one from work can bother you with job-related.</p>\n<p>It was only meant to be scheduled maintenance if I recall correctly, to upgrade a system in particular. For DevOps engineers, it is not a big deal. Who knows, this kind of not a big deal becomes a little hiccup at the beginning.</p>\n<p>Started from an unintended behavior after the changes been applied to the system. This might be not that accurate, but it is very intuitive for you to quickly resolve the unintended behavior when you make a change to a system right? When I found a defect in my development environment, I would debug it and make the best appropriate changes. It also goes for that DevOps engineer who just found an unintended behavior of a system after the changes have been applied by them.</p>\n<p>For the sake of accuracy, the quick fix is not working. I will be panic obviously, luckily it wasn’t me. The DevOps engineer then chats reports to the production incident report channel, letting everyone know that they are currently having a problem with a system, possibly impacting the whole system and degrading the whole service.</p>\n<p>Related team and upper management beginning to join up on to virtual war room. They want a clarification of what was happening and what was the situation. And most importantly how to quickly mitigate, hopefully, it is as easier as restarting the VM, a restart for all ultimate solution. But that was not the case.</p>\n<p>Related on-call engineers from different teams began to joining at around 9-11 pm. After they noticed degraded metrics on their watched service, a surprise for them founding that there was already a big virtual war room consisting of important people in the organization with all on-call engineers from different teams.</p>\n<p>Sadly, it is a long night for them. Their peaceful night has been gone. Time to work after work. But, it is the job right? We paid for it too.</p>\n<p>From the post mortem document I read, the team just figured the fix around 3-5 am. I don’t know whether it is considered mitigation or not, but the fix is the best possible and fastest way to recover from the degradation of the system. At least the entire system was serving as it is from 5-6 am, after all of the related on-call engineers from various teams perform their part to make a massive change on their watched services.</p>\n<p>I was not been in the virtual war room that night. The first time I realized what was happening is after I opened up my slack and found numerous notifications on the production incident report channel. Too bad I wasn’t there, too bad the incident degrading our system that entire night.</p>\n<p>I read the post mortem in the evening, to catch up on everything I need to know. And most importantly, to understand how should we handle a big incident like this costing numerous potential GMV.</p>\n<p>One thing for sure, life after that incident is not the same anymore. Any production-related activity is now requiring strict approval, the execution requiring a spectator, etc. And all of them come from the lessons learned from the incident, most importantly a well-documented post mortem and praiseworthy report of the DevOps.</p>","frontmatter":{"title":"Acing Your First On Call Schedule: Clear Report on Every Second","date":"January 10, 2021","description":null}},"previous":{"fields":{"slug":"/blog/49-circle-of-safety-feeling-safe-at-work-and-home/"},"frontmatter":{"title":"Circle of Safety: feeling safe at work and home"}},"next":{"fields":{"slug":"/blog/70-perfect-quality-of-software-is-impossible/"},"frontmatter":{"title":"Perfect Quality of Software is Impossible"}}},"pageContext":{"id":"4b9f08c1-c72c-53a5-bbb9-074cdb624698","previousPostId":"107d790b-87bf-5b81-9898-6ecc3a05cb57","nextPostId":"383fde72-9905-5be6-a2e3-ea465df317f6"}},
    "staticQueryHashes": ["2841359383"]}